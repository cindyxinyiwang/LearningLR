%\section{\label{sec:analysis} Analysis}
%In this section, we perform an analysis of exactly how DDS is learning, and what kind of data it is learning to select.

\begin{wrapfigure}{R}{0.3\textwidth}
  \vspace{-10mm}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{figs/cifar10_dds.eps}
  \end{center}
  \vspace{-4mm}
  \caption{\label{fig:dds_distribution}Class distributions of CIFAR-10 4K.}
  \vspace{-8mm}
\end{wrapfigure}

\subsection{Analysis of Image Classification}
 Prior work on heuristic based data selection has found that the model performs better if we feed higher quality data towards the end of training~\citep{dynamic_data_selection_nmt} or finetune the model with more relevant data~\citep{dynamic}. Here we verify this observation by analyzing the learned importance weight at the end of training for the image classification task. Figure \ref{fig:dds_distribution}, we show that at the end of training, the \dds learns to balance the class distribution, which is originally unbalanced due to the dataset creation. In Figure \ref{fig:dds_score}, we show that at the end of training, \dds assigns higher probabilities to images with clearer class content from ImageNet. These results show that \dds learns to focus on higher quality data towards the end of training.  

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/imagenet_dds.eps}
  \caption{\label{fig:dds_score} Example images from the ImageNet and their weights assigned by \dds. }
\end{figure}

\subsection{Analysis of Multilingual NMT}
Next, we focus on multilingual NMT, where the choice of data directly corresponds to picking a language, which has an intuitive interpretation. Since \dds adapts the data weights dynamically to the model throughout training, here we analyze how the learned weights change throughout training.
%\begin{center}
%  \includegraphics[width=0.245\columnwidth]{figs/aze_devppl_plot.eps}
%  \includegraphics[width=0.23\columnwidth]{figs/bel_devppl_plot.eps}
%  \includegraphics[width=0.23\columnwidth]{figs/glg_devppl_plot.eps}
%  \includegraphics[width=0.23\columnwidth]{figs/slk_devppl_plot.eps}
%  \captionof{figure}{\label{fig:nmt_converge}Development set perplexity vs. training steps. \textit{From left to right}: \texttt{aze}, \texttt{bel}, \texttt{glg}, \texttt{slk}.}
%\end{center}
%\paragraph{Training Curves.} First, we plot the dev set perplexity for three training methods, TCS, DDS, and TCS+DDS over the course of training in Figure \ref{fig:nmt_converge}.%
%\footnote{We did not plot ``Uniform'' here because it takes a far larger number of training steps to converge due to its uniform sampling of eight different languages, and thus is not visible on the same scale.}
%From the results, we can see that while DDS starts out with a higher perplexity than the TCS heuristics (which a-priori calculates the most appropriate language to be using based on surface statistics), it quickly catches up and surpasses TCS on all 4 languages.
%In addition, when initialized with the TCS heuristic, DDS starts at a relatively good perplexity and converges even faster.

\begin{center}
  \includegraphics[width=0.22\columnwidth]{figs/aze_hs_probs_plot.eps}
  \includegraphics[width=0.22\columnwidth]{figs/bel_hs_probs_plot.eps}
  \includegraphics[width=0.22\columnwidth]{figs/glg_hs_probs_plot.eps}
  \includegraphics[width=0.29\columnwidth]{figs/slk_hs_probs_plot.eps}
  \captionof{figure}{\label{fig:nmt_distrib_hs}Language usage for TCS$+$DDS by training step. \textit{From left to right}: \texttt{aze}, \texttt{bel}, \texttt{glg}, \texttt{slk}.}
\end{center}

%\paragraph{Learned Language Distributions.}
We plot the probability distribution of the four HRLs (because they have more data and thus larger impact on training) over the course of training.  Figure \ref{fig:nmt_distrib_hs} shows the change of language distribution for TCS+DDS. Since TCS selects the language with the largest vocabulary overlap with the LRL, the distribution is initialized to focus on the most related HRL. For all four LRLs, the percentage of their most related HRL starts to decrease as training continues. For \texttt{aze}, DDS quickly comes back to using its most related HRL. However, for \texttt{bel}, DDS continues the trend of using all four languages. This shows that DDS is able to maximize the benefits of the multilingual data by having a more balanced usage of all languages. 
%For gig and slk, DDS learns to mainly use both por and ces, their corresponding HRL.

\vspace{-0.2cm}
\begin{center}
  \includegraphics[width=0.22\columnwidth]{figs/aze_uni_probs_plot.eps}
  \includegraphics[width=0.22\columnwidth]{figs/bel_uni_probs_plot.eps}
  \includegraphics[width=0.22\columnwidth]{figs/glg_uni_probs_plot.eps}
  \includegraphics[width=0.29\columnwidth]{figs/slk_uni_probs_plot.eps}
  \captionof{figure}{\label{fig:nmt_distrib_uni}Language usage for DDS by training step. \textit{From left to right}: \texttt{aze}, \texttt{bel}, \texttt{glg}, \texttt{slk}.}
\end{center}
\vspace{-0.2cm}

In Figure \ref{fig:nmt_distrib_uni}, we show a more interesting trend of DDS without heuristic initialization.
For both \texttt{aze} and \texttt{bel}, our method learns to focus on their most related HRL after a certain number of training updates.
Interestingly, for \texttt{bel}, DDS learns to focus on both \texttt{rus}, its most related HRL, and another language \texttt{ces}. Similarly for \texttt{slk}, DDS also learns to focus on \texttt{ces}, its most related HRL, and \texttt{rus}, although there is little vocabulary overlap between \texttt{slk} and \texttt{rus}.
Also notably, the ratios significantly change over the course of training, indicating that different types of data may be more useful during different stages of learning the model.
Notably, DDS shows significant improvements over uniform sampling and other heuristics, demonstrating that it is able to discover when it should use other data, even if the patterns are not immediately intuitive.
%Similar to the trend in Figure \ref{fig:nmt_distrib_hs}, glg tends to use both por, its most related HRL, and ces. 
