\begin{table*}[t]
  \caption{\label{tab:results}Results for image classification accuracy (left) and multilingual MT BLEU (right). For MT, the statistical significance is indicated with $*$ (p $<$ 0.005) and $\dagger$ (p $<$ 0.0001). \dds~ outperforms the best baseline in all settings. For both image classification and NMT, \dds~performs better than other intelligent data selection methods.}
  \vspace{0.2cm}
   \begin{minipage}{.59\linewidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Methods}}
        & \multicolumn{2}{c}{CIFAR-10 (WRN-28-$k$)} & \multicolumn{2}{c}{ImageNet (ResNet-50)} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & 4K, $k=2$ & Full, $k=10$ & 10\% & Full \\
        \midrule
        Uniform &
        82.60$\pm$0.17 &
        95.55$\pm$0.15 & 
        56.36/79.45 &
        76.51/93.20 \\
        SPCL &
        81.09$\pm$0.22 &
        93.66$\pm$0.12 & 
        - &
        - \\
        BatchWeight &
        79.61$\pm$0.50 &
        94.11$\pm$0.18 & 
        - &
        - \\
        MentorNet &
        83.11$\pm$0.62 &
        94.92$\pm$0.34 & 
        - &
        - \\
        \midrule
        \dds     &
        83.63$\pm$ 0.29 &
        96.31$\pm$ 0.13 &
        \textbf{56.81}/\textbf{79.51} &
        \textbf{77.23}/\textbf{93.57} \\
         retrained \dds     &
        \textbf{85.56}$\pm$\textbf{0.20} &
        \textbf{97.91}$\pm$\textbf{0.12} &
        - &
        - \\       
        \bottomrule
      \end{tabular}
      }
      \end{minipage}
      \hfill
  \begin{minipage}{.4\linewidth}
    \resizebox{\textwidth}{!}{
      \begin{tabular}{l|cccc}
        \toprule
        \textbf{Methods} & \textbf{aze} & \textbf{bel} & \textbf{glg} & \textbf{slk} \\
        \midrule
        Uniform & 10.31 & 17.21 & 26.05 & 27.44 \\
        SPCL & 9.07 & 16.99 & 23.64 & 21.44 \\
        Related & 10.34 & 15.31 & 27.41 & 25.92 \\
        TCS     & 11.18 & 16.97 & 27.28 & 27.72 \\
        \midrule
        \dds     & 10.74 & 17.24 & 27.32 & $\mathbf{28.20^*}$ \\
        TCS+\dds & $\mathbf{11.84^*}$ & $\mathbf{17.74^\dagger}$ & \textbf{27.78} & 27.74 \\
        \bottomrule
      \end{tabular}
      }
      \end{minipage}
\end{table*}

\section{\label{sec:experiment}Experiments}
%\gn{You need at least one sentence of lead-in here.}
We now discuss experimental results on both image classification, an instance of the general classification problem using \autoref{alg:image_classification_dds}, and multilingual NMT using \autoref{alg:nmt_dds}.

\subsection{\label{exp:settings}Experimental Settings}

% \gn{One reviewer asked how the development sets are created. It seems like this needs to be mentioned here?} \cw{i added a footnote in the method section and included the details in appendix?} \gn{I found that now. Looks OK.}

\noindent \textbf{Data.} We apply our method on established benchmarks for image classification and multilingual NMT.
For image classification, we use CIFAR-10~\citep{cifar10} and ImageNet~\citep{imagenet}. For each dataset, we consider two settings: a reduced setting where only roughly 10\% of the training labels are used, and a full setting, where all labels are used. Specifically, the reduced setting for CIFAR-10 uses the first $4000$ examples in the training set, and with ImageNet, the reduced setting uses the first $102$ TFRecord shards as pre-processed by~\citet{imagenet_generalize_better}. We use the size of $224 \times 224$ for ImageNet.

For multilingual NMT, we use the 58-language-to-English TED dataset~\citep{ted_pretrain_emb}. 
Following prior work~\citep{ted_pretrain_emb,rapid_adapt_nmt,sde}, we evaluate translation from four low-resource languages~(LRL) Azerbaijani~(\texttt{aze}), Belarusian~(\texttt{bel}), Galician~(\texttt{glg}), and Slovak~(\texttt{slk}) to English, where each is paired with a similar high-resource language Turkish~(\texttt{tur}), Russian~(\texttt{rus}), Portugese~(\texttt{por}), and Czech~(\texttt{ces}) (details in \autoref{app:nmt_data}).
We combine data from all 8 languages, and use \dds~to optimize data selection for each LRL.

To update the scorer, we construct $\mathcal{D}_\text{dev}$ so that it does not overlap with $\mathcal{D}_\text{test}$. For image classification, we hold out $10\%$ of the training data as $\mathcal{D}_\text{dev}$; while for multilingual NMT, we simply use the dev set of the LRL as $\mathcal{D}_\text{dev}$.

\noindent \textbf{Models and Training Details.}
For image classification, on CIFAR-10, we use the pre-activation WideResNet-28~\citep{wide_res_net}, with width factor $k=2$ for the reduced setting and $k=10$ for the normal setting. For ImageNet, we use the post-activation ResNet-50~\citep{res_net}. 
%The batch sizes for CIFAR-10 and for ImageNet are $128$ and $4096$, running for 200K steps and 40K steps, respectively. 
%We use the standard Momentum update for the \dds~model parameter $\theta$, and the derived Momentum update rule in Section~\autoref{sec:grad_of_optimizers} for the \dds~distribution parameter $\psi$, both with the momentum rate of $0.9$.
These implementations reproduce the numbers reported in the literature~\citep{wide_res_net,res_net,resnext}, and additional details can be found in \autoref{app:image_hparam}.

For NMT, we use a standard LSTM-based attentional baseline \citep{attention}, which is similar to previous models used in low-resource scenarios on this dataset~\citep{rapid_adapt_nmt,sde} and others~\citep{lownmt19} due to its relative stability compared to other options such as the Transformer \citep{vaswani2017attention}. Accuracy is measured using BLEU score \citep{bleu}.
More experiment details are noted in \autoref{app:nmt_hparam}.

\noindent \textbf{Baselines and Our Methods.}
For both image classification and multi-lingual NMT, we compare the following data selection methods. \textbf{Uniform}: data is selected uniformly from all of the data that we have available, as is standard in training models. \textbf{SPCL}~\citep{spcl}: a curriculum learning method that dynamically updates the curriculum to focus more on the ``easy'' training examples based on model loss. \textbf{\dds}: our proposed method.
%\begin{itemize}
%\item \textbf{Uniform}: data is selected uniformly from all of the data that we have available, as is standard in training models. 
%\item \textbf{SPCL}~\citep{spcl}: a curriculum learning method that dynamically updates the curriculum to focus more on the ``easy'' training examples based on model loss.
%\item \textbf{\dds}: our proposed method.
%\end{itemize}

For image classification, we compare with several additional methods designed for filtering noisy data on CIFAR-10, where we simply consider the dev set as the clean data. \textbf{BatchWeight}~\citep{learn_reweight}: a method that scales example training loss in a batch with a locally optimized weight vector using a small set of clean data. \textbf{MentorNet}~\citep{mentornet}: a curriculum learning method that trains a mentor network to select clean data based on features from both the data and the main model.
%\begin{itemize}
%\item \textbf{BatchWeight}~\citep{learn_reweight}: a method that scales example training loss in a batch with a locally optimized weight vector using a small set of clean data. 
%\item  \textbf{MentorNet}~\citep{mentornet}: a curriculum learning method that trains a mentor network to select clean data based on features from both the data and the main model. 
%\end{itemize}


For machine translation, we also compare with two state-of-the-art heuristic methods for multi-lingual data selection. \textbf{Related}: data is selected uniformly from the target LRL and a linguistically related HRL \citep{rapid_adapt_nmt}. \textbf{TCS}: a recently proposed method of ``target conditioned sampling'', which uniformly chooses target sentences, then picks which source sentence to use based on heuristics such as word overlap \citep{TCS}. Note that both of these methods take advantage of structural properties of the multi-lingual NMT problem, and do not generalize to other problems such as classification. 
%\begin{itemize}
%\item \textbf{Related}: data is selected uniformly from the target LRL and a linguistically related HRL \citep{rapid_adapt_nmt}. 
%\item  \textbf{TCS}: a recently proposed method of ``target conditioned sampling'', which uniformly chooses target sentences, then picks which source sentence to use based on heuristics such as word overlap \citep{TCS}. Note that both of these methods take advantage of structural properties of the multi-lingual NMT problem, and do not generalize to other problems such as classification.
%\end{itemize}

%\gn{This is a little bit sudden to appear here directly in the experiments section. It might be good to mention this at the end of Section \ref{sec:efficient_reward} as a method to incorporate prior information? I think mentioning this there is actually a very good thing, as it adds to another thing that the proposed framework can do. If we mention it here, on the other hand, it seems a little bit like a hack that we added on at the last minute to make things work.}
\paragraph{DDS with Prior Knowledge} \dds~is a flexible framework to incorporate prior knowledge about the data using the scorer network, which can be especially important when the data has certain structural properties such as language or domain. We test such a setting of \dds~for both tasks.

For image classification, we use \textbf{retrained \dds}, where we first train a model and scorer network using the standard \dds~till convergence. The trained scorer network can be considered as a good prior over the data, so we use it to train the final model from scratch again using \dds. For multilingual NMT, we experiment with \textbf{TCS+\dds}, where we initialize the parameters of \dds~with the TCS heuristic, then continue training. 



% \noindent \textbf{Data Selection Baselines.} We compare our method against three strong baselines: 1) All: all 8 languages are used for training without any data selection; 2) Bi: we train on the combined datasets of each LRL and its related HRL, which is a special case of data selection that requires prior linguistic knowledge; 3) TCS~\citep{TCS}: the state-of-the-art data selection method for multilingual NMT. Given a target sentence, TCS conditionally samples a source sentence from the candidate languages based on simple heuristics such as vocabulary overlap.

%\gn{Just a comment: I'm not a huge fan of this because it seems a bit hacky, but if it's what was necessary to make things work this time then that's fair}. 
% We note that all these three tricks are not needed for our experiments on CIFAR-10, where the batch size is much smaller.

\subsection{Main Results}


%\begin{table}[t]
%  \captionof{table}{\label{tab:results}Results for image classification accuracy (left) and multilingual MT BLEU (right). For MT, the statistical significance is indicated with $*$ (p < 0.005) and $\dagger$ (p < 0.0001).}
%  \begin{minipage}{.575\linewidth}
%    \resizebox{\textwidth}{!}{
%      \begin{tabular}{lcccc}
%        \toprule
%        \multirow{2}{*}{\textbf{Methods}}
%        & \multicolumn{2}{c}{CIFAR-10 (WRN-28-$k$)} & \multicolumn{2}{c}{ImageNet (ResNet-50)} \\
%        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%        & 4K, $k=2$ & Full, $k=10$ & 10\% & Full \\
%        \midrule
%        Uniform &
%        82.60$\pm$0.17 &
%        95.55$\pm$0.15 & 
%        56.36/79.45 &
%        76.51/93.20 \\
%        SPCL &
%        81.09$\pm$0.22 &
%        93.66$\pm$0.12 & 
%        - &
%        - \\
%        BatchWeight &
%        79.61$\pm$0.50 &
%        94.11$\pm$0.18 & 
%        - &
%        - \\
%        MentorNet &
%        83.11$\pm$0.62 &
%        94.92$\pm$0.34 & 
%        - &
%        - \\
%        \midrule
%        \dds     &
%        83.63$\pm$ 0.29 &
%        96.31$\pm$ 0.13 &
%        \textbf{56.81}/\textbf{79.51} &
%        \textbf{77.23}/\textbf{93.57} \\
%         retrained \dds     &
%        \textbf{85.56}$\pm$\textbf{0.20} &
%        \textbf{97.91}$\pm$\textbf{0.12} &
%        - &
%        - \\       
%        \bottomrule
%      \end{tabular}
%    }
%  \end{minipage}
%  \begin{minipage}{.425\linewidth}
%    \resizebox{\textwidth}{!}{
%      \begin{tabular}{l|cccc}
%        \toprule
%        \textbf{Methods} & \textbf{aze} & \textbf{bel} & \textbf{glg} & \textbf{slk} \\
%        \midrule
%        Uniform & 10.31 & 17.21 & 26.05 & 27.44 \\
%        SPCL & 9.07 & 16.99 & 23.64 & 21.44 \\
%        Related & 10.34 & 15.31 & 27.41 & 25.92 \\
%        TCS     & 11.18 & 16.97 & 27.28 & 27.72 \\
%        \midrule
%        \dds     & 10.74 & 17.24 & 27.32 & $\mathbf{28.20^*}$ \\
%        TCS+\dds & $\mathbf{11.84^*}$ & $\mathbf{17.74^\dagger}$ & \textbf{27.78} & 27.74 \\
%        \bottomrule
%      \end{tabular}
%    }
%  \end{minipage}
%  \vspace{-4mm}
%\end{table}

% \begin{wraptable}{r}{0.6\textwidth}
% %\begin{adjustbox}{max width=0.7\textwidth}
% \vspace{-1cm}
% \resizebox{0.6\textwidth}{!}{
%   \begin{tabular}{llll}
%     \multicolumn{4}{c}{\textbf{CIFAR-10} ($\text{mean} \pm \text{std}$ over $10$ runs)} \\
%   \toprule
%     \textbf{Portion} &
%     \textbf{Model} &
%     \textbf{Baseline} &
%     \textbf{\\dds}
%     \\
%   \midrule
%     4K &
%     WideResNet-28-2 &
%     $82.60 \pm 0.17$ & % 6109331
%     $\mathbf{83.63 \pm 0.29}$ % 6109486
%     \\
%     Full &
%     WideResNet-28-10 &
%     $95.55 \pm 0.15$ &  % 6153155
%     $\mathbf{96.31 \pm 0.13}$  % 6170380
%     \\
%   \bottomrule
%     \\
%     \multicolumn{4}{c}{\textbf{ImageNet} ($\text{Top-1}/\text{Top-5}$)} \\
%     \toprule
%     \textbf{Portion} &
%     \textbf{Model} &
%     \textbf{Baseline} &
%     \textbf{\\dds}
%     \\
%   \midrule
%     10\%  & ResNet-50 &
%     $56.36 / 79.45$ & % 5267621/workUnits/118
%     $\mathbf{56.81 / 79.51}$   % 6020481/workUnits/18
%     \\  
%     Full & ResNet-50 &
%     $76.51 / 93.20$ & % https://arxiv.org/abs/1810.12890
%     $\mathbf{77.23 / 93.57}$ % 6800564/workUnits/39
%     \\
%     \bottomrule
%   \end{tabular}
% }
%   \captionof{table}{\label{tab:image_classification_results}Image classification accuracy. Higher is better.}
% %\end{adjustbox}
% %\end{center}
% \end{wraptable}

% \noindent \textbf{Results.} Results are presented in Table~\autoref{tab:image_classification_results}. As can be seen, \\dds~improves the performance of all tasks considered. The best improvement is for CIFAR-10 using 4K labels. We posit that this is because when the  
%\gn{The following explanation is pretty weak because increasing the degrees of freedom of the model was not even one of our original motivations. Is there any way we can at least say something about how it (might be?) reducing the weight on outliers or something like that?} In our intuitions, \\dds~provides an extra degree of freedom to train the models, namely the per-example scaling of gradients. This extra degree of freedom is well-utilized by the $p(\hat{x}, \hat{y}; \psi)$ distribution, leading to the improvements.


% \subsection{Multilingual NMT}
% 
% \noindent \textbf{Baselines.} We compare our method against three strong baselines: 1) All: all 8 languages are used for training without any data selection; 2) Bi: we train on the combined datasets of each LRL and its related HRL, which is a special case of data selection that requires prior linguistic knowledge; 3) TCS~\citep{TCS}: the state-of-the-art data selection method for multilingual NMT. Given a target sentence, TCS conditionally samples a source sentence from the candidate languages based on simple heuristics such as vocabulary overlap.
% 
% \noindent \textbf{Implementation.} Here we clarify several design choices for Algorithm \autoref{alg:nmt_\dds}. To model the distribution $p(S_i|y;\psi)$, we use a 2-layer feed-forward network with hidden size of 32. The input vector for the network is a vector of size of source languages $n$, representing which of the languages contain a corresponding source sentence for a given target sentence $y$. We use the standard Adam update rule with learning rate of 0.001 for the NMT model parameter $\theta$, and the derived Adam update rule from Section~\autoref{sec:grad_of_optimizers} with learning rate~0.0001 for the distribution parameter $\psi$. 
% 
% We test two different settings for using \dds for multilingual  NMT: 1) TCS+\dds: we pretrain the network with the heuristic distribution from TCS before the \dds training process
% %. K in Algorithm \autoref{alg:nmt_dds} is set to be 50,000; 
% 2) Uniform+DDS: we train the network for $\psi$ from scratch. %At the start of training, K is set to be 5,000 to encourage exploration; after updating $\psi$ for 5 times, we also set it to 50,000.
% We run all experiments with 3 different random seeds and pick the median for each method. Additional hyperparameters are listed in Appendix~\autoref{app:nmt_hparam}.
% 
% \noindent \textbf{Results.}
% \begin{wraptable}{l}{0.6\textwidth}
% \begin{center}
% \vspace{-0.5cm}
%     \begin{tabular}{l|cccc}
%     \toprule
%     \textbf{Model} & \textbf{aze} & \textbf{bel} & \textbf{glg} & \textbf{slk} \\
%     \midrule
%     All & 10.31 & 17.21 & 26.05 & 27.44 \\
%     Bi & 10.34 & 15.31 & 27.41 & 25.92 \\
%     TCS & 11.18 & 16.97 & 27.28 & 27.72 \\
%     \midrule
%     TCS+DDS & \textbf{11.84} & \textbf{17.74} & \textbf{27.78} & 27.74 \\
%     %Uniform & 9.54 & 14.75 & 25.11 & 26.30 \\
%     Uniform+DDS & 10.74 & 17.24 & 27.32 & \textbf{28.20} \\
%     \bottomrule
%     \end{tabular}
%      \captionof{table}{\label{tab:nmt_result}BLEU score. Higher is better.}
% \end{center}
% \vspace{-0.5cm}
% \end{wraptable}

The results of the baselines and our method are listed in \autoref{tab:results}.
First, comparing the standard baseline strategy of ``Uniform'' and the proposed method of ``\dds'' we can see that in all 8 settings \dds~improves over the uniform baseline. This is a strong indication of both the consistency of the improvements that \dds~can provide, and the generality -- it works well in two very different settings. Next, we find that \dds~outperforms SPCL by a large margin for both of the tasks, especially for multilingual NMT. This is probably because SPCL weighs the data only by their easiness, while ignoring their relevance to the dev set, which is especially important in settings where the data in the training set can have very different properties such as the different languages in multilingual NMT. 

\dds~also brings improvements over the state-of-the-art intelligent data utilization methods. For image classification, \dds~outperforms MentorNet and BatchWeight on CIFAR-10 in all settings. For NMT, in comparison to Related and TCS, vanilla \dds~performs favorably with respect to these state-of-the-art data selection baselines, outperforming each in 3 out of the 4 settings (with exceptions of slightly underperforming Related on \texttt{glg} and TCS on \texttt{aze}).
In addition, we see that incorporating prior knowledge into the scorer network leads to further improvements. For image classification, retrained \dds~can significantly improve over regular \dds, leading to the new state-of-the-art result on the CIFAR-10 dataset. For mulitlingual NMT, TCS+\dds~achieves the best performance in three out of four cases (with the exception of \texttt{slk}, where vanilla \dds~already outperformed TCS).\footnote{Significance tests~\citep{significance_nmt} find significant gains over the baseline for \texttt{aze}, \texttt{slk}, and \texttt{bel}. For \texttt{glg}, \dds~without heuristics performs as well as the TCS baseline.}

\dds~does not incur much computational overhead. For image classification and multilingual NMT respectively, the training time is about $1.5\times$ and $2\times$ the regular training time without \dds. This contrasts favorably to previous methods that learn to select data using reinforcement learning. For example, in the IMDB movie review experiment in  \citet{learn_what_data_learn}, the data agent is trained for 200 episode, where each episode uses around 40\% of the whole dataset, requiring 80x more training time than a single training run. 
%\footnote{The code for multilingual NMT is not optimized, so its training time could be reduced further}.