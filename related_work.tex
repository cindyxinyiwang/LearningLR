\section{\label{sec:related_work}Related Work}
Our method is related to the general machine learning problem of teaching. First, difficulty-based teaching in curriculum learning estimates the curriculum based on a heuristic understanding of the hardness of data~\citep{cl_bengio,SpitkovskyAJ10,baysian_curriculum,zhang2016boosting,automate_cl_GravesBMMK17,zhang2018empirical,platanios19naacl}. These methods, though effective, are harder to generalize because they often require task-specific difficulty measures. On the other hand, self-paced learning~\citep{spl_kumar,spl_visual_category} defines the hardness of the data based on the loss from the main model, but it is still based on the assumption that easy examples should be learned first. Our method is probably closest to the learning to teach framework~\citep{learn_to_teach} but their formulation involves complicated feature design and the optimization using RL is expensive. Instead, we formulate our reward using bi-level optimization, which has been used for a variety of other tasks~\citep{bilevel_optim,hier_optim,darts,hyper_grad,learn_reweight}.

Several pieces of prior work have focused on data selection for domain adaptation for many different tasks~\citep{moore2010intelligent,axelrod2011domain,domain_adapt_transfer,jiang-zhai-2007-instance,foster-etal-2010-discriminative,wang-etal-2017-instance}, generally using heuristics to measure domain similarity.
%\cite{domain_adapt_transfer} propose to estimate the importance weight of the classification labels in the pretraining dataset to mitigate the domain differences, while \dds~is a more general data selection framework that works for both classification and other usage cases.
Besides domain adaptation, it has also been found that selecting good examples from the training data can improve NMT in the face of noisy or otherwise undesirable data~\citep{vyas-etal-2018-identifying,pham-etal-2018-fixing}. In addition, submodular optimization~\citep{submodular_mt,learn_mix_submodular} selects training data that are similar to dev set, but the criterion is often based on hand-designed features and the data usage is predefined before training. Our method is also related to works on training instance weighting~\citep{importance_weight,learn_reweight,jiang-zhai-2007-instance,domain_adapt_transfer} which reweigh data based on an estimated or derived weight vector, instead of using a parameterized neural network.
\gn{Here we should discuss \cite{learn_reweight} in detail, so that we can explain the difference for people who are familiar with this paper. Specifically (1) our work is parameterized and parameters are amortized over the entire dataset, which may be essential in preventing the overfitting mentioned in Section \ref{diff_data_selection}, (2) our work is not only focused at filtering noisy data, and (3) it empirically performs better.}
\cite{reinforce_cotrain,rl_nmt,learn_active_learn} proposes RL frameworks for specific natural language processing tasks, but their methods are less generalizable and requires more complicated featurization.   
%The idea of selecting training data that are similar to dev data has been used in works on submodular optimization~\citep{submodular_mt,learn_mix_submodular}, but they rely on features specific to the task, while \dds~directly optimizes the the dev set performance, and is generalizable across tasks. Moreover, unlike \dds, these methods cannot adaptively change the data selection scheme. \cite{importance_weight} optimizes data weights by minimizing the error rate on the dev set. However, they use a single number to weigh each subgroup of augmented data, and their algorithm requires an expensive heuristic method to update data weights; while \dds~uses a more expressive parameterized neural network to model the individual data weights, which are efficiently updated by directly differentiating the dev set loss.




%Instance weighting: \cite{jiang-zhai-2007-instance,foster-etal-2010-discriminative,wang-etal-2017-instance}. In particular, \cite{wang-etal-2017-instance} seems like a good paper to compare against because it's recent and based on neural MT.

%Curriculum learning: \cite{zhang2016boosting,zhang2018empirical,platanios19naacl}.

%Data selection: \cite{moore2010intelligent,axelrod2011domain}.

%Removing bad training examples improves MT: \cite{vyas-etal-2018-identifying,pham-etal-2018-fixing}

%Dataset poisoning (for neural models): \cite{koh2017understanding}



%Meta-learning. Formulation of using the gradient update equation is similar to MAML \cite{finn2017model}.
