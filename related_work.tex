\section{\label{sec:related_work}Related Works}
%\gn{These are mostly based on stuff for NMT, a broader survey is necessary to increase more general ML methods. I've also only added a small subset of the papers on MT. Please follow the backward and forward references (forward references can be found by clicking the ``cited by'' link in Google Scholar).}

For both many different tasks, several pieces of prior work have focused on data selection for domain adaptation~\citep{moore2010intelligent,axelrod2011domain,domain_adapt_transfer,jiang-zhai-2007-instance,foster-etal-2010-discriminative,wang-etal-2017-instance}, generally using heuristics to measure domain similarity.
\cite{domain_adapt_transfer} propose to estimate the importance weight of the classification labels in the pretraining dataset to mitigate the domain differences, while DDS is a more general data selection framework that works for both classification and other usage cases.
Besides domain adaptation, it has also been found that selecting good examples from the training data can improve NMT in the face of noisy or otherwise undesirable data~\citep{vyas-etal-2018-identifying,pham-etal-2018-fixing}.  

The formulation of DDS involves bilevel optimization~\citep{bilevel_optim,hier_optim}, which has been utilized in several prior works in areas other than data selection~\citep{darts,hyper_grad,finn2017model}.
Notably, \cite{darts} proposes a nested optimization to link training and dev set, similar to the general philosophy behind DDS, but utilizes this formulation for efficient neural architecture search, while DDS is designed for efficient data usage.

More generally, our method is also related to the general machine learning problem of teaching. First, hardness-based teaching in curriculum learning estimates the curriculum based on a heuristic understanding of the hardness of data~\citep{cl_bengio,automate_cl_GravesBMMK17,SpitkovskyAJ10,zhang2016boosting,zhang2018empirical,platanios19naacl,baysian_curriculum}. These methods, though effective, are harder to generalize because they often require task-specific and heuristic difficulty measures. On the other hand, self-paced learning~\citep{spl_visual_category,spl_kumar,spl_visual_category} defines the hardness of the data based on the loss from the main model. This method requires fewer heuristic estimates, but is still based on the assumption that easy examples should be learned first. Secondly, the recently proposed learning to teach~\citep{learn_to_teach} is probably closer to our setting, where they train a teacher model so that it can ``teach'' the main model better. However, their method requires featurizing the data and student models so that the teacher model could be trained with reinforcement learning. In comparison, our method focuses on data selection, and provides a much simpler framework without feature engineering. 

%Lastly, our method is related to the work on investigating the effect of label noise on deep image recognition models \cite{rolnick2017deep,overfit_random_examples} or machine translation \cite{khayrallah-koehn-2018-impact}, while \citet{koh2017understanding} shows the feasibility of training set poisoning attacks.


%Instance weighting: \cite{jiang-zhai-2007-instance,foster-etal-2010-discriminative,wang-etal-2017-instance}. In particular, \cite{wang-etal-2017-instance} seems like a good paper to compare against because it's recent and based on neural MT.

%Curriculum learning: \cite{zhang2016boosting,zhang2018empirical,platanios19naacl}.

%Data selection: \cite{moore2010intelligent,axelrod2011domain}.

%Removing bad training examples improves MT: \cite{vyas-etal-2018-identifying,pham-etal-2018-fixing}

%Dataset poisoning (for neural models): \cite{koh2017understanding}



%Meta-learning. Formulation of using the gradient update equation is similar to MAML \cite{finn2017model}.
