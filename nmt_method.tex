\subsection{\label{sec:nmt_method}Formulation for Multilingual NMT}

Multilingual training is shown to be very effective at improve the translation quality of low-resource langauges~\citep{nmt_transfer,rapid_adapt_nmt,johnson_nmt}. In practice, we have a multilingual corpus $\mathcal{D}_{\text{train}}$ of $n$ source languages $(S_1, S_2, ..., S_n)$ to the target language $T$. \cite{rapid_adapt_nmt} shows that training a single NMT model on the concatenation of both the low-resource language and its most related high-resource langauge identified with linguistic knowledge is generally efficient and very competitive. However, intead of relying on linguistic knowledge, here we want to automatically select the optimal data from $\mathcal{D}_{\text{train}}$ for training an NMT model of the language $S$ to $T$. We want to train a parameterized distribution $p(x, y;\psi)$ with a support over $\mathcal{D}_{\text{train}}$, such that selecting training data according to this distribution leads to the optimal NMT model $\theta^*$ on the language $S$. 
%That is, $\psi$ satisfies the following condition
%\begin{equation}
%  \label{eqn:nmt_argmin}
%  \begin{aligned}
%    \arg\min_\psi
%    \sum_{x_i, y_i \in S\text{-}Y} \ell_{\text{dev}}(x_i, y_i; \theta^*)
%    ~~~\text{where}~~~
%    \theta^* = \arg\min_\theta \mathbb{E}_{x_i,y_i \sim p(X, Y;\psi)}\left[ \ell_{\text{train}}(x_i, y_i; \theta) \right]
%  \end{aligned}
%\end{equation}
%\begin{wraptable}{l}{8cm}
    %\vspace{-0.3cm}
\begin{center}
\resizebox{!}{3.5cm}{
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetCommentSty{itshape}
\SetKwComment{Comment}{$\triangleright$\ }{}
%\KwResult{Write here the result }

\Input{$\mathcal{D}_{\text{train}}$; K: number of data to train the NMT model before updating $\psi$; 
E: number of updates for $\psi$; 
$\alpha_1$,$\alpha_2$: discount factors for the gradient}

\Output{The converged NMT model $\theta^*$}

  Initialize $\psi_0$, $\theta_0$
  
  \Comment{Initialize the gradient of each source language}
  $grad[S_i] \leftarrow 0$ \textbf{for} \textit{i in n}
  %\For{ i in n}{
  %
  %  $grad[S_i] \leftarrow 0$
  %  
  %}
 
  \While{$\theta$ not converged}{
    %\Comment{Sample training data according to $\psi$}
    $X, Y \leftarrow \text{load\_data}(\psi, \mathcal{D}_{\text{train}}, K)$  \label{alg:load_nmt}
  
    \Comment{Train the NMT model}
    \For{ $x_i, y$ in $X, Y$}{
      $\theta_t \leftarrow \text{GradientUpdate}\left( \theta_{t-1}, \nabla_{\theta_{t-1}} \ell(x_i, y; \theta_{t-1}) \right)$
        
      $grad[S_i] \leftarrow \alpha_1 \times \text{grad}[S_i] + \alpha_2 \times \nabla_{\theta_{t-1}} \ell(x_i, y; \theta_{t-1})$
    }
  
    \Comment{Optimize $\psi$}
    \For{ iter in E}{
      
      sample $B$ data pairs from $\mathcal{D}_{\text{train}}$
      
      $d_\psi \leftarrow \frac{1}{B} \sum_{j=1}^B \sum_{i=1}^n \left[ \text{grad}[S_i]^\top \text{grad}[S] \cdot \nabla_{\psi_{t-1}} \text{log}\left( p\left( S_i|y_j;\psi_{t-1} \right) \right) \right]$
       
      $\psi_t \leftarrow \text{GradientUpdate}(\psi_{t-1}, d_{\psi_{t-1}})$ 
    }
  }
  \caption{\label{alg:nmt_dds}Training multilingual NMT with DDS.}
\end{algorithm}
}
\end{center}
    \vspace{-0.2cm}
%\end{wraptable} 

To make training more efficient and stable for multilingual NMT, we make several modifications of the main framework in section \ref{sec:diff_data_selection}: 1) we normalize $p(\psi)$ as a distribution over the source languages, given a target sentence and its corresponding multilingual source sentences; this design follows the formulation of Target Conditioned Sampling~(TCS), the existing state-of-the-art data selection method that samples a source language for each target sentence based on the vocabulary overlap between the source langauges $S_i$ and $S$, the language we want to model~\citep{TCS}; 2) we only update $\psi$ after updating the NMT model for a fixed number of steps; 3) for multilingual NMT, the dev set $\mathcal{D}_{\text{dev}}$ is a set of parallel data from $S$-$T$. Therefore, we can replace $\ell(\mathcal{D}_{\text{dev}})$ in Equation \ref{eqn:two_step_update} with loss of the training data from $S$-$T$; 4) we sample the data according to $p(\psi)$ to get a Monte Carlo estimate of the objective in Equation \ref{eqn:psi_theta_argmin}. This can significantly reduce the training time compared to using all data. The pseudo code of the training process is in Algorithm \ref{alg:nmt_dds}.

%Note that in Line \ref{alg:load_nmt} of Algorithm \ref{alg:nmt_dds}, we load $K$ training data according to $p(\psi)$. Since we formulate $p(\psi)$ as $p(S_i|y;\psi)$, the data loading procedure is the same as the TCS algorithm~\citep{TCS}: for each of the $K$ target sentences, we calculate a distribution over its source languages according to $p(S_i|y;\psi)$, and then sample the corresponding source sentences based on this distribution.

 
