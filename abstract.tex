\begin{abstract}
Good training data matters. While deep learning models in general benefit from a large amount of training data, low-quality training data such as noisy, out-of-domain, or adversarial data are detrimental to the learning of these models. Meanwhile, identifying the optimal set of training instances is a challenging problem, since the effect of any given set of data usually only becomes visible at the end of the training, which is a long and expensive process. In this paper, we propose Differentiable Data Selection~(\dds), a novel method for selecting high-quality training data. The selection process is parameterized as a differentiable function, which can be efficiently trained along with the model that consumes the selected data. On two modalities, we show that without significant computing overhead, \dds~delivers strong and consistent improvements. Specifically, on multilingual machine translation, \dds~can identify which related languages are helpful to improve the translation of another language, leading to consistent improvements over a strong heuristic data selection baseline. On image classification tasks with CIFAR-10 and ImageNet, and with different ranges of data, \dds~can also identify the importance of the training instance, leading to consistent improvements over the baselines in all settings. Code will be available up on acceptance.\footnote{If our paper is accepted, but we do not submit our code along with the camera-ready version, we will courteously retract the accepted paper.}

\end{abstract}