\begin{abstract}
Good training data matters. While deep learning models in general benefit from a large amount of training data, low-quality training data such as noisy, out-of-domain, or adversarial data
%\paul{is out of domain really low quality? seems more like this is acknowledging that we want to overfit to the domain of the test set... I would just remove this item}
%\cw{i guess out-of-domain might not be low-quality, but too much of it is detrimental to the main task. For multilingual MT, related languages can also be seen as out-of-domain data}
% \paul{go from koh & liang and follow citation trail for adversarial data}
% \cw{do we need to add citation in the abstract? maybe just do it in related work}
are detrimental to the learning of these models. Meanwhile, identifying the optimal set of training instances is a challenging problem, since the effect of any given set of data usually only becomes visible at the end of the training, which is a long and expensive process. In this paper, we propose Differentiable Data Selection~(\dds), a novel method for selecting high-quality training data. The selection process is parameterized as a differentiable function, which can be efficiently trained along with the model that consumes the selected data. On two modalities, we show that without significant computing overhead, \dds~delivers strong and consistent improvements. Specifically, on multilingual machine translation, \dds~can identify which related languages are helpful to improve the translation of another language, leading to consistent improvements over a strong heuristic data selection baseline. On image classification tasks with CIFAR-10 and ImageNet, and with different ranges of data, \dds~can also identify the importance of the training instance, leading to consistent improvements over the baselines in all settings. Code will be available up on acceptance.\footnote{If our paper is accepted, but we do not submit our code along with the camera-ready version, we will courteously retract the accepted paper.}

\end{abstract}